# CPU

- [CPU](#cpu)
  - [CPU组成](#cpu组成)
  - [CPU流水线](#cpu流水线)
  - [流水线停顿](#流水线停顿)
  - [操作数前推](#操作数前推)
  - [分支预测](#分支预测)
  - [超线程](#超线程)
  - [指令集](#指令集)
  - [高速缓存](#高速缓存)
  - [局部性原理](#局部性原理)

## CPU组成

![CPU组成](https://github.com/gongluck/images/blob/main/计算机基础/CPU组成.png)

## CPU流水线

![CPU流水线](https://github.com/gongluck/images/blob/main/计算机基础/CPU流水线.png)

- 用来同步时钟周期的，不再是指令级别，而是流水线阶段级别。每一级流水线对应的输出，都要放到流水线寄存器(Pipeline Register)里面，然后在下一个时钟周期，交给下一个流水线级去处理。每增加一级的流水线，就要多一级写入到流水线寄存器的操作。
- 流水线技术并不能缩短单条指令的响应时间这个性能指标，但是可以增加在运行很多条指令时候的吞吐率。因为不同的指令，实际执行需要的时间是不同的。
- 而流水线带来的吞吐率提升，只是一个理想情况下的理论值。在实践的应用过程中，还需要解决指令之间的依赖问题。要想解决好冒险的依赖关系问题，需要引入乱序执行、分支预测等技术。

## 流水线停顿

![CPU流水线停顿](https://github.com/gongluck/images/blob/main/计算机基础/CPU流水线停顿.png)

- 如果发现了后面执行的指令，会对前面执行的指令有数据层面的依赖关系，最简单的办法就是"再等等"。在进行指令译码的时候，会拿到对应指令所需要访问的寄存器和内存地址。所以这个时候能够判断出来，这个指令是否会触发数据冒险。如果会触发数据冒险，就可以决定，让整个流水线停顿一个或者多个周期。

## 操作数前推

![操作数前推](https://github.com/gongluck/images/blob/main/计算机基础/操作数前推.png)

- 操作数前推，就是通过在硬件层面制造一条旁路，让一条指令的计算结果，可以直接传输给下一条指令，而不再需要"指令1写回寄存器，指令2再读取寄存器"这样多此一举的操作。这样直接传输带来的好处就是，后面的指令可以减少，甚至消除原本需要通过流水线停顿，才能解决的数据冒险问题。
- 在流水线里，后面的指令不依赖前面的指令，那就不用等待前面的指令执行，它完全可以先执行。

## 分支预测

![分支预测](https://github.com/gongluck/images/blob/main/计算机基础/分支预测.png)
![预测错误](https://github.com/gongluck/images/blob/main/计算机基础/预测错误.png)

- 最简单的分支预测技术，叫作"假装分支不发生"。就是仍然按照顺序，把指令往下执行。其实就是CPU预测，条件跳转一定不发生。这样的预测方法，也是一种静态预测技术。
- 如果分支预测是正确的，节省下来本来需要停顿下来等待的时间。如果分支预测失败了，那就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作`Zap`或者`Flush`。CPU不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，还需要做对应的清除操作。清空已经使用的寄存器里面的数据等等，这些清除操作，也有一定的开销。
- CPU需要提供对应的丢弃指令的功能，通过控制信号清除掉已经在流水线中执行的指令。只要对应的清除开销不要太大，就是划得来的。
- 现代流水线CPU通常会采用一种名为"投机执行"的方式来优化条件跳转指令的执行。所谓投机执行，是指CPU会通过分析历史的分支执行情况，来推测条件跳转指令将会执行的分支，并提前处理所预测分支上的指令。而等到CPU发现之前所预测的分支是错误的时候，将不得不丢弃这个分支上指令的所有中间处理结果，并将执行流程转移到正确的分支上。很明显，这样就会浪费较多的时钟周期。

## 超线程

![超线程核心](https://github.com/gongluck/images/blob/main/计算机基础/超线程核心.png)

- 超线程的CPU，把一个物理层面CPU核心，伪装成两个逻辑层面的CPU核心。会在硬件层面增加很多电路，使可以在一个CPU核心内部，维护两个不同线程的指令的状态信息。
- 在CPU的其他功能组件上，Intel不会提供双份。无论是指令译码器还是ALU，一个CPU核心仍然只有一份。因为超线程并不是真的去同时运行两个指令，那就真的变成物理多核了。超线程的目的，是在一个线程A的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU的译码器和ALU就空出来了，那么另外一个线程B，就可以拿来干自己需要的事情。这个线程B可没有对于线程A里面指令的关联和依赖。

## 指令集

![CPU指令集](https://github.com/gongluck/images/blob/main/计算机基础/CPU指令集.png)

- CPU的指令集里的机器码是固定长度还是可变长度，也就是复杂指令集(Complex Instruction Set Computing，简称CISC)和精简指令集(Reduced Instruction Set Computing，简称 RISC)。

## 高速缓存

![CPU高速缓存](https://github.com/gongluck/images/blob/main/计算机基础/CPU高速缓存.png)
![CPU缓存行](https://github.com/gongluck/images/blob/main/计算机基础/CPU缓存行.png)

- 为了弥补CPU与内存两者之间的性能差异，能真实地把CPU的性能提升用起来，而不是让它在那儿空转，在现代CPU中引入了高速缓存。
- 在CPU里，通常会有L1、L2、L3这样三层高速缓存。
- 每个CPU核心都有一块属于自己的L1高速缓存，通常分成指令缓存和数据缓存，分开存放CPU使用的指令和数据。
- L2的Cache同样是每个CPU核心都有的，不过往往不在CPU核心的内部。所以L2的访问速度会比L1稍微慢一些。
- L3通常是多个CPU核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。
- 从CPU Cache被加入到现有的CPU里开始，内存中的指令、数据，会被加载到L1-L3 Cache中，而不是直接由CPU访问内存去拿。在95%的情况下，CPU都只需要访问L1-L3 Cache，从里面读取指令和数据，而无需访问内存。
- CPU从内存中读取数据到CPU Cache的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在CPU Cache里面，叫作Cache Line。
- 在CPU Cache里，对于数据的写入，有写直达和写回这两种解决方案。写直达把所有的数据都直接写入到主内存里面，简单直观，但是性能就会受限于内存的访问速度。而写回则通常只更新缓存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到主内存里。在缓存经常会命中的情况下，性能更好。
- 上下文切换要做的和异常和中断里的是一样的。上下文切换的过程，需要把当前执行线程的寄存器等等的信息，保存到线程栈里面。而这个过程也必然意味着，已经加载到高速缓存里面的指令或者数据，又回到了主内存里面，会进一步拖慢性能。

## 局部性原理

- 时间局部性。如果一个数据被访问了，那么它在短时间内还会被再次访问。
- 空间局部性。如果一个数据被访问了，那么和它相邻的数据也很快会被访问。
- 有了时间局部性和空间局部性，不用再把所有数据都放在内存里，也不用都放在硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。
