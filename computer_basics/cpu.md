# CPU

- [CPU](#cpu)
  - [CPU 组成](#cpu-组成)
  - [CPU 流水线](#cpu-流水线)
  - [分支预测](#分支预测)
  - [超线程](#超线程)
  - [指令集](#指令集)
  - [高速缓存](#高速缓存)
  - [局部性原理](#局部性原理)
  - [用户态线程模型](#用户态线程模型)
  - [中断](#中断)
  - [性能分析](#性能分析)

## CPU 组成

![CPU组成](https://github.com/gongluck/images/blob/main/cpu/structure.png)

## CPU 流水线

![CPU流水线](https://github.com/gongluck/images/blob/main/cpu/pipeline.png)

- 用来同步时钟周期的，不再是指令级别，而是流水线阶段级别。每一级流水线对应的输出，都要放到流水线寄存器(Pipeline Register)里面，然后在下一个时钟周期，交给下一个流水线级去处理。每增加一级的流水线，就要多一级写入到流水线寄存器的操作。
- 流水线技术并不能缩短单条指令的响应时间这个性能指标，但是可以增加在运行很多条指令时候的吞吐率。因为不同的指令，实际执行需要的时间是不同的。
- 而流水线带来的吞吐率提升，只是一个理想情况下的理论值。在实践的应用过程中，还需要解决指令之间的依赖问题。要想解决好冒险的依赖关系问题，需要引入乱序执行、分支预测等技术。
- 如果发现了后面执行的指令，会对前面执行的指令有数据层面的依赖关系，最简单的办法就是"再等等"。在进行指令译码的时候，会拿到对应指令所需要访问的寄存器和内存地址。所以这个时候能够判断出来，这个指令是否会触发数据冒险。如果会触发数据冒险，就可以决定，让整个流水线停顿一个或者多个周期。
- 在流水线里，后面的指令不依赖前面的指令，那就不用等待前面的指令执行，它完全可以先执行。

## 分支预测

![分支预测](https://github.com/gongluck/images/blob/main/cpu/branch_prediction.png)
![预测错误](https://github.com/gongluck/images/blob/main/cpu/prediction_error.png)

- 最简单的分支预测技术，叫作"假装分支不发生"。就是仍然按照顺序，把指令往下执行。其实就是 CPU 预测，条件跳转一定不发生。这样的预测方法，也是一种静态预测技术。
- 如果分支预测是正确的，节省下来本来需要停顿下来等待的时间。如果分支预测失败了，那就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作`Zap`或者`Flush`。CPU 不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，还需要做对应的清除操作。清空已经使用的寄存器里面的数据等等，这些清除操作，也有一定的开销。
- CPU 需要提供对应的丢弃指令的功能，通过控制信号清除掉已经在流水线中执行的指令。只要对应的清除开销不要太大，就是划得来的。
- 现代流水线 CPU 通常会采用一种名为"投机执行"的方式来优化条件跳转指令的执行。所谓投机执行，是指 CPU 会通过分析历史的分支执行情况，来推测条件跳转指令将会执行的分支，并提前处理所预测分支上的指令。而等到 CPU 发现之前所预测的分支是错误的时候，将不得不丢弃这个分支上指令的所有中间处理结果，并将执行流程转移到正确的分支上。很明显，这样就会浪费较多的时钟周期。

## 超线程

![超线程核心](https://github.com/gongluck/images/blob/main/cpu/hyper_threaded_core.png)

- 超线程的 CPU，把一个物理层面 CPU 核心，伪装成两个逻辑层面的 CPU 核心。会在硬件层面增加很多电路，使可以在一个 CPU 核心内部，维护两个不同线程的指令的状态信息。
- 在 CPU 的其他功能组件上，Intel 不会提供双份。无论是指令译码器还是 ALU，一个 CPU 核心仍然只有一份。因为超线程并不是真的去同时运行两个指令，那就真的变成物理多核了。
- 超线程的目的，是在一个线程 A 的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU 的译码器和 ALU 就空出来了，那么另外一个线程 B，就可以拿来干自己需要的事情。这个线程 B 可没有对于线程 A 里面指令的关联和依赖。

## 指令集

![CPU指令集](https://github.com/gongluck/images/blob/main/cpu/architecture.png)

- CPU 的指令集里的机器码是固定长度还是可变长度，也就是复杂指令集(Complex Instruction Set Computing，简称 CISC)和精简指令集(Reduced Instruction Set Computing，简称 RISC)。

## 高速缓存

![CPU高速缓存](https://github.com/gongluck/images/blob/main/cpu/cache.png)
![CPU缓存行](https://github.com/gongluck/images/blob/main/cpu/cache_line.png)

- 为了弥补 CPU 与内存两者之间的性能差异，能真实地把 CPU 的性能提升用起来，而不是让它在那儿空转，在现代 CPU 中引入了高速缓存。
- 在 CPU 里，通常会有 L1、L2、L3 这样三层高速缓存。
- 每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成指令缓存和数据缓存，分开存放 CPU 使用的指令和数据。
- L2 的 Cache 同样是每个 CPU 核心都有的，不过往往不在 CPU 核心的内部。所以 L2 的访问速度会比 L1 稍微慢一些。
- L3 通常是多个 CPU 核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。
- 从 CPU Cache 被加入到现有的 CPU 里开始，内存中的指令、数据，会被加载到 L1-L3 Cache 中，而不是直接由 CPU 访问内存去拿。在 95%的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存。
- CPU 从内存中读取数据到 CPU Cache 的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在 CPU Cache 里面，叫作 Cache Line。
- 在 CPU Cache 里，对于数据的写入，有写直达和写回这两种解决方案。写直达把所有的数据都直接写入到主内存里面，简单直观，但是性能就会受限于内存的访问速度。而`写回则通常只更新缓存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到主内存里`。在缓存经常会命中的情况下，性能更好。
- `上下文切换要做的和异常和中断里的是一样的。上下文切换的过程，需要把当前执行线程的寄存器等等的信息，保存到线程栈里面。而这个过程也必然意味着，已经加载到高速缓存里面的指令或者数据，又回到了主内存里面，会进一步拖慢性能。`

## 局部性原理

- 时间局部性。如果一个数据被访问了，那么它在`短`时间内还会被再次访问。
- 空间局部性。如果一个数据被访问了，那么和它`相邻`的数据也很快会被访问。
- 有了时间局部性和空间局部性，不用再把所有数据都放在内存里，也不用都放在硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。

## 用户态线程模型

- 一对一模型，一个用户级别线程对应一个内核级别线程，可能会影响系统的性能。即使一个用户线程发生阻塞，也不会影响该进程的执行。
- 多对一模型，创建多个用户级线程，但实际上只有一个内核级线程在运行，并没有实现并行。
- 多对多(两级线程)模型，允许多个用户级线程复用到更小或者数量相同内核级线程上，可以创建任意多的用户级线程，而相应的内核级线程可以在多核处理器上正常运行。当一个线程被阻塞，内核也会调度其他的线程来处理。

## 中断

- Linux 中的中断处理程序分为上半部和下半部：
  - 上半部对应硬件中断，用来快速处理中断。
  - 下半部对应软中断，用来异步处理上半部未完成的工作。
- Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的运行情况。

## 性能分析

![CPU性能分析](https://github.com/gongluck/images/blob/main/linux/performance/cpu.png)

- CPU 绑定：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。
- CPU 独占：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。
- 优先级调整：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。
- 为进程设置资源限制：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。
- NUMA（Non-Uniform Memory Access）优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。
- 中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。
